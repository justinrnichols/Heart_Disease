# -*- coding: utf-8 -*-
"""heart_disease_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TILSnwqXU4AJN4O3NH2fSwlRI7nLw5oG

Import necessary libraries.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split, KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

"""Data comprises of 14 attributes that describe the presence of heart disease in a patient. Dataset was found from the source Kaggle and UCI Machine Learning.

Link: https://www.kaggle.com/ronitf/heart-disease-uci

Load dataset into dataframe.
"""

url = 'https://drive.google.com/uc?export=download&id=1xHAX81G7Q6IcWqbm84QsQdnxuADvrT-8'
df = pd.read_csv(url)
model_accuracies = {}

"""Show the first 5 elements of the dataset."""

df.head()

"""Compute some basic descriptive information about the data."""

df.describe()

"""Show the correlation among the different attributes."""

df.corr()

"""Correleation plots using a Heat Map and a Trellis Plot."""

sns.heatmap(df.corr(),annot=True,fmt='.1f')
sns.PairGrid(df).map(plt.scatter).add_legend()

"""Obtain X and y values from dataframe."""

X = df.iloc[:,:-1].values
y = df.iloc[:,-1].values

"""Split the data into training and testing sets, an 80, 20 split respectively. """

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

"""Scale the data to avoid the model not converging.

"""

X_scaled = StandardScaler()
X_train = X_scaled.fit_transform(X_train)
X_test = X_scaled.transform(X_test)

"""##Linear Regression

Linear Regression (intercept=True) prediction.
"""

lin_reg = LinearRegression(fit_intercept=True)
lin_reg.fit(X_train,y_train)
y_pred = lin_reg.predict(X_test)
print('Test accuracy {:.2f}%'.format(lin_reg.score(X_test,y_test)*100))
model_accuracies["Linear Regression: fit_intercept=True"] = lin_reg.score(X_test,y_test)

"""Linear Regression (intercept=True) plot."""

plt.title(f'Linear Regression (intercept=True) Predictions r = {round(lin_reg.score(X_test, y_test), 3)}')
plt.scatter(X_test[:,0], y_pred)
plt.xlabel('Actual diagnosis')
plt.ylabel('Predicted diagnosis')
plt.show()

"""## Logical Regression

Logical Regression (intercept=False) prediction.
"""

log_reg = LogisticRegression(fit_intercept=False)
log_reg.fit(X_train,y_train)
y_pred = log_reg.predict(X_test)
print('Test accuracy {:.2f}%'.format(log_reg.score(X_test,y_test)*100))
model_accuracies["Logical Regression: fit_intercept=False"] = accuracy_score(y_test, y_pred)

"""Logical Regression (intercept=False) plot."""

plt.title(f'Logical Regression (intercept=False) Predictions r = {round(log_reg.score(X_test, y_test), 3)}')
plt.scatter(X_test[:,0], y_pred, c=y_test, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Actual diagnosis')
plt.ylabel('Predicted diagnosis')
plt.show()

"""Performance metrics: precision, recall, f1-score, support."""

print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

"""Logical Regression (intercept=True) prediction."""

log_reg = LogisticRegression(fit_intercept=True)
log_reg.fit(X_train,y_train)
y_pred = log_reg.predict(X_test)
print('Test accuracy {:.2f}%'.format(log_reg.score(X_test,y_test)*100))
model_accuracies["Logical Regression: fit_intercept=True"] = accuracy_score(y_test, y_pred)

"""Logical Regression (intercept=True) plot."""

plt.title(f'Logical Regression (intercept=True) Predictions r = {round(log_reg.score(X_test, y_test), 3)}')
plt.scatter(X_test[:,0], y_pred, c=y_test, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Actual diagnosis')
plt.ylabel('Predicted diagnosis')
plt.show()

"""Performance metrics: precision, recall, f1-score, support."""

print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

"""##KNN

KNN (n_neighbors=5) prediction.
"""

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train,y_train)
y_pred = knn.predict(X_test)
print('Test accuracy {:.2f}%'.format(knn.score(X_test,y_test)*100))
model_accuracies["K-Nearest Neighbor: n_neighbors=5"] = accuracy_score(y_test, y_pred)

"""Performance metrics: precision, recall, f1-score, support."""

print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

"""KNN (n_neighbors=8) prediction."""

knn = KNeighborsClassifier(n_neighbors=8)
knn.fit(X_train,y_train)
y_pred = knn.predict(X_test)
print('Test accuracy {:.2f}%'.format(knn.score(X_test,y_test)*100))
model_accuracies["K-Nearest Neighbor: n_neighbors=8"] = accuracy_score(y_test, y_pred)

"""Performance metrics: precision, recall, f1-score, support."""

print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

"""KNN (n_neighbors=10) prediction."""

knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(X_train,y_train)
y_pred = knn.predict(X_test)
print('Test accuracy {:.2f}%'.format(knn.score(X_test,y_test)*100))
model_accuracies["K-Nearest Neighbor: n_neighbors=10"] = accuracy_score(y_test, y_pred)

"""Performance metrics: precision, recall, f1-score, support."""

print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

"""##Decision Tree

Decision Tree (criterion='gini') prediction.
"""

tree = DecisionTreeClassifier(criterion='gini')
tree.fit(X_train,y_train)
y_pred = tree.predict(X_test)
print('Test accuracy {:.2f}%'.format(tree.score(X_test,y_test)*100))
model_accuracies["Decision Tree: criterion='gini'"] = accuracy_score(y_test, y_pred)

"""Performance metrics: precision, recall, f1-score, support."""

print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

"""Decision Tree (criterion='entropy') prediction."""

tree = DecisionTreeClassifier(criterion='entropy')
tree.fit(X_train,y_train)
y_pred = tree.predict(X_test)
print('Test accuracy {:.2f}%'.format(tree.score(X_test,y_test)*100))
model_accuracies["Decision Tree: criterion='entropy'"] = accuracy_score(y_test, y_pred)

"""Performance metrics: precision, recall, f1-score, support."""

print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

"""## SVC

SVC (kernel='linear, C=[0.001, 0.01, 0.1, 1]) prediction.
"""

y_preds = []
for c in [0.001, 0.01, 0.1, 1]:
  svc = SVC(kernel='linear', C=c)
  svc.fit(X_train, y_train)
  y_pred = svc.predict(X_test)
  y_preds.append(y_pred)
  print('Test accuracy (c=' + str(c) + ') {:.2f}%'.format(svc.score(X_test,y_test)*100))
  model_accuracies["SVC: C=" + str(c)] = accuracy_score(y_test, y_pred)
y_preds = np.array(y_preds)

"""Performance metrics: precision, recall, f1-score, support."""

x,y = y_preds.shape
for p in range(x):
  print(classification_report(y_test, y_preds[p]))
  print(accuracy_score(y_test, y_preds[p]))

"""Accuracy score for each model and hyper-parameter sorted from highest to lowest."""

print('Models sorted by accuracy score:')
model_accuracies = dict(reversed(sorted(model_accuracies.items(), key=lambda item: item[1])))
for m in model_accuracies:
  print('{}: {:.2f}%'.format(m, model_accuracies[m] * 100))